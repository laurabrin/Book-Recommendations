{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Browsing Books - A Recomendation Engine for Books\n",
    "For CMPT3520 Machine Learning II <br/>\n",
    "Annabell Rodriguez, Laura Brin, Sandra Alex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most recommendation systems, book recommendations are rooted in commerce and what you are suggested is often based on data collected specifically about you. Ecommerce venues like Amazon, Chapters and Google play books look at what books have you purchased before, what authors and genres you like, what are other people with similar literary tastes are reading. These all go into recommending books for a specific person and can be extemely useful in ecommerce or on book rating sites like goodreads and bookish. \n",
    "\n",
    "Public libraries have the same concept, but in aggregate. There are over a million books published around the world each year and along with a resource selection criterion, a book recommendation system could help libraries understand what their readers may want.  In understanding their own regular patrons, libraries can better predict and select new books that will be popular in their areas. This may highlight a customer market for genres, languages, or age groups. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two datasets are used for this solution. The first is a book recommendation dataset from Kaggle. It contains 3 files: books, users, and rating. The books file has roughly 270,000 books identified by ISBN, book title, author, year of publication, publisher, and thumbnail images for the book cover on Amazon. The ratings file has over a million ratings. The user file contains the age and location for 278858 users. The data is sourced from bookcrossing.com. The second dataset is from Github user Zygmunt Zajac with additions from Olivier Simard-Hanley with added fields for description, number of pages and genres for 50,000 books. This data is sourced from goodbooks.com. \n",
    "\n",
    "\n",
    "https://github.com/zygmuntz/goodbooks-10k<br/>\n",
    "https://github.com/malcolmosh/goodbooks-10k-extended<br/> \n",
    "https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset<br/>\n",
    "\n",
    "Additionally, some code formatting is taken from a Kaggle notebook by user Vishorita<br/>\n",
    "https://www.kaggle.com/code/vishorita/best-recommendation-collabarative-filtering \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy in any of its forms is hard to assess with recommendation systems without having ground truth labels. While similarity measures like cosine or Euclidean can tell us which clusters exist, they cannot tell us if they are meaningful. For this project we will be using a smoke test: selecting a book, or list of books we have existing knowledge of to see if recommended books have expected titles or authors. \n",
    "\n",
    "Content based filtering: Content based filtering has advantages over collaborative when it comes to online predictions of new books. The model does not rely on finding relationships between users or ratings, instead finding relationships in the NLP processing for similar words, genres, or authors. \n",
    "\n",
    "Collaborative Filtering: Recommendations are much simpler for offline prediction where the books have been rated by multiple users. It still faces issues with sparsity, so we use an ensemble approach to identify books first from a matrix of books with a higher number of user ratings and more active users, and then from the sparse matrix. Online predictions for new books are more difficult than new users. New users will be able to receive recommendations that will start either poorly or randomly and improve as additional knowledge is added implicitly or explicitly. New books, unless very popular and quickly reviewed, will start quite sparse and may take time to generate on to recommendation lists for users. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browsing Books"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import collections\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.manifold\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Add some convenience functions to Pandas DataFrame.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "def mask(df, key, function):\n",
    "  \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n",
    "  return df[function(df[key])]\n",
    "\n",
    "def flatten_cols(df):\n",
    "  df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "  return df\n",
    "\n",
    "pd.DataFrame.mask = mask\n",
    "pd.DataFrame.flatten_cols = flatten_cols\n",
    "\n",
    "# Install Altair and activate its colab renderer.\n",
    "#print(\"Installing Altair...\")\n",
    "#!pip install git+git://github.com/altair-viz/altair.git\n",
    "#!pip install altair vega_datasets\n",
    "import altair as alt\n",
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "alt.renderers.enable('colab')\n",
    "#print(\"Done installing Altair.\")\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "from ast import literal_eval\n",
    " \n",
    "books_small = pd.read_csv('https://raw.githubusercontent.com/malcolmosh/goodbooks-10k/master/books_enriched.csv', index_col=[0],dtype={\"isbn\":object}, converters={\"genres\": literal_eval, \"authors\": literal_eval})\n",
    "#note books_enriched.csv is a modified version of books.csv from goodbooks-10k dataset\n",
    "users = pd.read_csv(\"Datasets\\\\Users.csv\")\n",
    "ratings= pd.read_csv(\"Datasets\\\\Ratings.csv\")\n",
    "books_large = pd.read_csv(\"Datasets\\\\Books.csv\",dtype={\"ISBN\":object},low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_large.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books_large.shape)\n",
    "print(books_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books_large.columns)\n",
    "print(books_small.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small = books_small[books_small['language_code'] == 'eng'].copy()\n",
    "books_small = books_small[[\"authors\", \"description\", \"genres\", \"isbn\", \"original_title\", \"original_publication_year\"]].copy()\n",
    "books_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of books by number of genres\n",
    "books_small['genres'].value_counts().groupby(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small[\"isbn\"].isin(ratings[\"ISBN\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small[\"isbn\"].isin(books_large[\"ISBN\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small[\"original_title\"].isin(books_large[\"Book-Title\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_large[\"Book-Title\"].isin(books_small[\"original_title\"]).value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books=pd.merge(books_large,books_small,how=\"inner\",right_on=\"original_title\",left_on=\"Book-Title\")\n",
    "all_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books.drop(['index','Book-Title','isbn13','books_count','publishDate','isbn','language_code','Book-Author', 'Year-Of-Publication', 'Publisher','small_image_url','Image-URL-S','Image-URL-M','Image-URL-L'],axis=1,inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books[\"ISBN\"].isin(ratings[\"ISBN\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding column for consecutive number by duplicate book-title and year\n",
    "all_books['duplicate_count'] = all_books.groupby(['original_title', all_books['authors'].map(tuple)]).cumcount()+1\n",
    "all_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['city'],users['state'],users['country']=users[\"Location\"].str.split(\",\",2).str\n",
    "users.drop(['Location'],axis=1,inplace=True, errors='ignore')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"country\"].fillna(\"Unknown\", inplace = True)\n",
    "users[\"state\"].fillna(\"Unknown\", inplace = True)\n",
    "users[\"city\"].fillna(\"Unknown\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list=users[\"country\"].value_counts().where(users[\"country\"].value_counts()>7500,other=\"Other\")\n",
    "print(country_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"new_country\"]=users[\"country\"].apply(lambda x: x if country_list[x]!=\"Other\" else \"Other\")\n",
    "users[\"new_country\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list=users[\"state\"].value_counts().where(users[\"state\"].value_counts()>5000,other=\"Other\")\n",
    "users[\"new_state\"]=users[\"state\"].apply(lambda x: x if state_list[x]!=\"Other\" else \"Other\")\n",
    "users[\"new_state\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=users[\"city\"].value_counts().where(users[\"city\"].value_counts()>1500,other=\"Other\")\n",
    "users[\"new_city\"]=users[\"city\"].apply(lambda x: x if city_list[x]!=\"Other\" else \"Other\")\n",
    "users[\"new_city\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "f=lambda x: x if x<100 else round(random.randint(24,44))\n",
    "users['Age']=users['Age'].apply(f)\n",
    "users.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.describe(include=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.drop(['city','state','country'],axis=1,inplace=True, errors='ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort column list authors\n",
    "all_books['authors']=all_books['authors'].apply(lambda x: sorted(x))\n",
    "all_books['authors_str'] = all_books['authors'].apply(lambda x: ' '.join(map(str, x)))\n",
    "all_books = all_books.copy()\n",
    "all_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_books=pd.merge(ratings,all_books,how=\"inner\",on=\"ISBN\")\n",
    "ratings_books=pd.merge(ratings_books,all_books[all_books['duplicate_count'] == 1],how=\"inner\", on=[\"original_title\",\"original_publication_year\", \"authors_str\"])\n",
    "ratings_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_books['ISBN']=ratings_books.apply(lambda x: x['ISBN_y'] if x['duplicate_count_x']>1 and x['duplicate_count_y']==1 else x['ISBN_x'],axis=1)\n",
    "ratings_books.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing when duplicate_count greater than 1\n",
    "all_books=all_books[all_books['duplicate_count'] == 1]\n",
    "#removing column duplicate_count\n",
    "all_books.drop(['duplicate_count'],axis=1,inplace=True, errors='ignore')\n",
    "all_books.reset_index(drop=True, inplace=True)\n",
    "all_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books['Book_id']=all_books.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substituting ISBN in ratings_books with book_id\n",
    "ratings_books=pd.merge(ratings_books,all_books,how=\"left\",left_on=\"ISBN\",right_on=\"ISBN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns except User-ID\tISBN\tBook-Rating\n",
    "ratings_books = ratings_books[['User-ID', 'Book_id','Book-Rating']]\n",
    "ratings_books.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_books.drop_duplicates(inplace=True)\n",
    "ratings_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_books.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize genres, books are assigned a random genre from their genre list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books_final = all_books.copy()\n",
    "filter=lambda x: random.choice(x)\n",
    "all_books_final['genre_rnd']=all_books['genres'].apply(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books_final.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books_final['genre_rnd'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ratings = all_books_final.merge(\n",
    "    ratings_books\n",
    "    .groupby(\"Book_id\", as_index=False)\n",
    "    .agg({'Book-Rating': ['count', 'mean']})\n",
    "    .flatten_cols(),\n",
    "    on='Book_id')\n",
    "\n",
    "genre_filter = alt.selection_multi(fields=['genre_rnd'])\n",
    "\n",
    "genre_chart = alt.Chart().mark_bar().encode(\n",
    "    x=\"count()\",\n",
    "    y=alt.Y('genre_rnd'),\n",
    "    color=alt.condition(\n",
    "        genre_filter,\n",
    "        alt.Color(\"genre_rnd:N\"),\n",
    "        alt.value('lightgray'))\n",
    ").properties(height=600, selection=genre_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(book_ratings[['Book_id', 'Book-Rating count', 'Book-Rating mean']]\n",
    " .sort_values('Book-Rating count', ascending=False)\n",
    " .head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_hist(field, label, filter):\n",
    "  \"\"\"Creates a layered chart of histograms.\n",
    "  The first layer (light gray) contains the histogram of the full data, and the\n",
    "  second contains the histogram of the filtered data.\n",
    "  Args:\n",
    "    field: the field for which to generate the histogram.\n",
    "    label: String label of the histogram.\n",
    "    filter: an alt.Selection object to be used to filter the data.\n",
    "  \"\"\"\n",
    "  base = alt.Chart().mark_bar().encode(\n",
    "      x=alt.X(field, bin=alt.Bin(maxbins=10), title=label),\n",
    "      y=\"count()\",\n",
    "  ).properties(\n",
    "      width=300,\n",
    "  )\n",
    "  return alt.layer(\n",
    "      base.transform_filter(filter),\n",
    "      base.encode(color=alt.value('lightgray'), opacity=alt.value(.7)),\n",
    "  ).resolve_scale(y='independent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of ratings and average rating per book.\n",
    "alt.hconcat(\n",
    "    filtered_hist('Book-Rating count', '# ratings / book', genre_filter),\n",
    "    filtered_hist('Book-Rating mean', 'mean rating', genre_filter),\n",
    "    genre_chart,\n",
    "    data=book_ratings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the rows with duplicate book titles\n",
    "duplicate_title_rows = all_books_final[all_books_final.duplicated(subset=['original_title', 'authors_str'], keep=False)]\n",
    "\n",
    "# Sort the duplicate rows by book title for easier inspection\n",
    "duplicate_title_rows = duplicate_title_rows.sort_values(by='original_title')\n",
    "\n",
    "# Print the list of books with duplicate titles\n",
    "print(\"List of books with duplicate titles:\\n\")\n",
    "print(duplicate_title_rows[['original_title', 'authors_str']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = all_books_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['genres_str'] = books['genres'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['original_title', 'authors_str', 'genres_str', 'description']\n",
    "features[1:]\n",
    "books[features[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize text\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Reconstruct the text\n",
    "    cleaned_text = ' '.join(words)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def combine_features(row, features=['original_title', 'authors_str', 'genres_str', 'description']):\n",
    "    #print(row)\n",
    "    result = row[features[0]]\n",
    "    for feature in features[1:]:\n",
    "        result += ' ' + str(row[feature])        \n",
    "    return result\n",
    "\n",
    "# Clean and preprocess the text features\n",
    "features = ['original_title', 'authors_str', 'genres_str', 'description']\n",
    "books['combined_features'] = books.apply(combine_features, args=(features,), axis=1)\n",
    "books['cleaned_combined_features'] = books['combined_features'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(books['cleaned_combined_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_content(title, cosine_sim_internal=cosine_sim, top_n=5):\n",
    "    # Get the index of the book that matches the title\n",
    "    idx = books[books['original_title'] == title].index\n",
    "\n",
    "    if len(idx) == 0:\n",
    "        print(\"Book not found in dataset\")\n",
    "        return None\n",
    "\n",
    "    # Get the pairwise similarity scores of all books with that book\n",
    "    sim_scores = list(enumerate(cosine_sim_internal[idx[0]]))\n",
    "\n",
    "    # Sort the books based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the top_n most similar books (excluding the input book itself)\n",
    "    sim_scores = sim_scores[0:top_n + 1]\n",
    "\n",
    "    # Get the book indices\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top_n most similar books\n",
    "    return books.iloc[book_indices][['original_title', 'authors', 'genres', 'description', 'cleaned_combined_features']]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = recommend_books_content(\"Fantastic Beasts and Where to Find Them\")\n",
    "recommended_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = recommend_books_content(\"The Princess Bride\")\n",
    "recommended_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = recommend_books_content(\"The Da Vinci Code\")\n",
    "recommended_books"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing without the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the text features\n",
    "features = ['original_title', 'authors_str', 'genres_str']\n",
    "books['combined_features_2'] = books.apply(combine_features, args=(features,), axis=1)\n",
    "books['cleaned_combined_features_2'] = books['combined_features_2'].apply(clean_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_2 = TfidfVectorizer()\n",
    "tfidf_matrix_2 = tfidf_vectorizer_2.fit_transform(books['cleaned_combined_features_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_2 = cosine_similarity(tfidf_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = recommend_books_content(\"Fantastic Beasts and Where to Find Them\", cosine_sim_internal=cosine_sim_2)\n",
    "recommended_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = recommend_books_content(\"The Princess Bride\", cosine_sim_internal=cosine_sim_2)\n",
    "recommended_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = recommend_books_content(\"The Da Vinci Code\", cosine_sim_internal=cosine_sim_2)\n",
    "recommended_books"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With user preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume the user has read and liked the following books:\n",
    "user_preferences = ['The Fault in Our Stars', 'Pride and Prejudice', 'Memoirs of a Geisha']\n",
    "\n",
    "# Extract the cleaned_combined_features of these books\n",
    "user_profile = books[books['original_title'].isin(user_preferences)]['cleaned_combined_features'].tolist()\n",
    "\n",
    "# Combine the features of the user's preferred books\n",
    "user_profile_combined = ' '.join(user_profile)\n",
    "\n",
    "# Calculate the cosine similarity between the user profile and all books\n",
    "tfidf_user_profile = tfidf_vectorizer.transform([user_profile_combined])\n",
    "cosine_sim_user = cosine_similarity(tfidf_user_profile, tfidf_matrix)\n",
    "\n",
    "# Get the top N recommendations\n",
    "top_n = 5\n",
    "book_indices = cosine_sim_user.argsort().flatten()[-top_n:]\n",
    "recommended_books = books.iloc[book_indices][['original_title', 'authors']]\n",
    "\n",
    "print(\"Top\", top_n, \"book recommendations for the user:\")\n",
    "print(recommended_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the cleaned_combined_features of these books\n",
    "user_profile_2 = books[books['original_title'].isin(user_preferences)]['cleaned_combined_features_2'].tolist()\n",
    "\n",
    "# Combine the features of the user's preferred books\n",
    "user_profile_combined_2 = ' '.join(user_profile_2)\n",
    "\n",
    "# Calculate the cosine similarity between the user profile and all books\n",
    "tfidf_user_profile_2 = tfidf_vectorizer_2.transform([user_profile_combined_2])\n",
    "cosine_sim_user_2 = cosine_similarity(tfidf_user_profile_2, tfidf_matrix_2)\n",
    "\n",
    "# Get the top N recommendations\n",
    "top_n = 5\n",
    "book_indices = cosine_sim_user_2.argsort().flatten()[-top_n:]\n",
    "recommended_books = books.iloc[book_indices][['original_title', 'authors']]\n",
    "\n",
    "print(\"Top\", top_n, \"book recommendations for the user:\")\n",
    "print(recommended_books)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Based Recommender System"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix factorization collaborative filtering provides recommendations to users based on neighbours with similar rating patterns identified by preference or rating. For user based, a similarity measure it ranks the opinion of closer clusters of neighbours to be of more value than those at a farther distance. Essentially the proximity is used to add weights to other users’ ratings which are then normalized and aggregated to determine a predicted rating and rank for books for the target user. </br>\n",
    "\n",
    "For item-based filtering, the resulting matrix of users and books helps determine similarity of books not based on content but rather on the behaviour of the users. This can be helpful when combined with content-based filtering to provide books that the user may like but may not have been exposed to before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_large.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = ratings.merge(books_large, on=['ISBN'], how='inner')\n",
    "print(complete_df.shape)\n",
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pt(users_min_ratings=200, books_min_ratings=50):\n",
    "    x = complete_df.groupby('User-ID').count()['Book-Rating']>users_min_ratings\n",
    "    knowledgable_users = x[x].index\n",
    "\n",
    "    filtered_rating = complete_df[complete_df['User-ID'].isin(knowledgable_users)]\n",
    "\n",
    "    y = filtered_rating.groupby('Book-Title').count()['Book-Rating']>=books_min_ratings\n",
    "    famous_books = y[y].index\n",
    "\n",
    "    final_ratings =  filtered_rating[filtered_rating['Book-Title'].isin(famous_books)]\n",
    "\n",
    "    pt = final_ratings.pivot_table(index='Book-Title',columns='User-ID'\n",
    "                          ,values='Book-Rating')\n",
    "\n",
    "    pt.fillna(0,inplace=True)\n",
    "\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = create_pt()\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_sparse = csr_matrix(pt)\n",
    "book_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = cosine_similarity(book_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_collaborative(title, pt_internal=pt, cosine_sim_internal=similarity_score, top_n=5):\n",
    "    # Get the index of the book that matches the title in pt\n",
    "    index = np.where(pt_internal.index==title)[0]\n",
    "\n",
    "    if len(index) == 0:\n",
    "        print(\"Book not found in dataset\")\n",
    "        return None\n",
    "\n",
    "    similar_books = sorted(list(enumerate(cosine_sim_internal[index[0]])),key=lambda x:x[1], reverse=True)[0:top_n+2]\n",
    "\n",
    "    # Get the book indices\n",
    "    book_indices = []\n",
    "    \n",
    "    for i in similar_books:\n",
    "        item = []\n",
    "        book_indices.append((books_large[books_large['Book-Title'] == pt_internal.index[i[0]]]).head(1).index[0])\n",
    "\n",
    "\n",
    "    #book_indices = [i[0] for i in similar_books]\n",
    "    \n",
    "\n",
    "    # Return the top_n most similar books\n",
    "    return books_large.iloc[book_indices][['Book-Title', 'Book-Author', 'Image-URL-M']]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(pt.index==\"1984\")[0]\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(enumerate(similarity_score[index[0]])),key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative(\"Fantastic Beasts and Where to Find Them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative(\"The Princess Bride\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative(\"The Da Vinci Code\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = NearestNeighbors(algorithm='brute')\n",
    "model_knn.fit(book_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_collaborative_knn(title, pt_internal=pt, model=model_knn, top_n=5):\n",
    "    # Get the index of the book that matches the title in pt\n",
    "    index = np.where(pt_internal.index==title)[0]\n",
    "\n",
    "    if len(index) == 0:\n",
    "        print(\"Book not found in dataset\")\n",
    "        return None\n",
    "\n",
    "    distances, suggestions = model.kneighbors(pt_internal.iloc[index[0], :].values.reshape(1, -1))\n",
    "\n",
    "    # Get the book indices\n",
    "    book_indices = []\n",
    "    \n",
    "    total = top_n\n",
    "    for i in suggestions[0]:\n",
    "        if total == 0:\n",
    "            break\n",
    "        item = []\n",
    "        book_indices.append((books_large[books_large['Book-Title'] == pt_internal.index[i]]).head(1).index[0])\n",
    "        total = total - 1\n",
    "\n",
    "    # Return the top_n most similar books\n",
    "    return books_large.iloc[book_indices][['Book-Title', 'Book-Author', 'Image-URL-M']]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative_knn(\"Fantastic Beasts and Where to Find Them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative_knn(\"The Princess Bride\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative_knn(\"The Da Vinci Code\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_new = create_pt(10, 10)\n",
    "pt_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_sparse_new = csr_matrix(pt_new)\n",
    "book_sparse_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score_new = cosine_similarity(book_sparse_new)\n",
    "similarity_score_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative(\"Fantastic Beasts and Where to Find Them\", pt_internal=pt_new, cosine_sim_internal=similarity_score_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative(\"The Princess Bride\", pt_internal=pt_new, cosine_sim_internal=similarity_score_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative(\"The Da Vinci Code\", pt_internal=pt_new, cosine_sim_internal=similarity_score_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn_new = NearestNeighbors(algorithm='brute')\n",
    "model_knn_new.fit(book_sparse_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative_knn(\"Fantastic Beasts and Where to Find Them\", pt_internal=pt_new, model=model_knn_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative_knn(\"The Princess Bride\", pt_internal=pt_new, model=model_knn_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_collaborative_knn(\"The Da Vinci Code\", pt_internal=pt_new, model=model_knn_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoke tests indicate that an ensemble approach would be best at providing accurate book recommendations. Content based filtering with book descriptions should be cross referenced by a list generated without recommendations to remove erroneous suggestions. Additionally, by mixing collaborative and content-based recommendations, it is more likely that you can recommend something to a user they may not have received via a single calculation. By creating sparse lists, the model can perform calculations faster, but at the sacrifice of books with low counts of ratings. This may result in a return of no results, which can be mitigated with a content recommended list.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
