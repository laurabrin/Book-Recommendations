{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Browsing Books - A Recomendation Engine for Books\n",
    "For CMPT3520 Machine Learning II <br/>\n",
    "Annabell Rodriguez, Laura Brin, Sandra Alex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browsing Books"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.manifold\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Add some convenience functions to Pandas DataFrame.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "def mask(df, key, function):\n",
    "  \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n",
    "  return df[function(df[key])]\n",
    "\n",
    "def flatten_cols(df):\n",
    "  df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "  return df\n",
    "\n",
    "pd.DataFrame.mask = mask\n",
    "pd.DataFrame.flatten_cols = flatten_cols\n",
    "\n",
    "# Install Altair and activate its colab renderer.\n",
    "#print(\"Installing Altair...\")\n",
    "#!pip install git+git://github.com/altair-viz/altair.git\n",
    "#!pip install altair vega_datasets\n",
    "import altair as alt\n",
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "alt.renderers.enable('colab')\n",
    "#print(\"Done installing Altair.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "from ast import literal_eval\n",
    " \n",
    "books_small = pd.read_csv('https://raw.githubusercontent.com/malcolmosh/goodbooks-10k/master/books_enriched.csv', index_col=[0],dtype={\"isbn\":object}, converters={\"genres\": literal_eval})\n",
    "#note books_enriched.csv is a modified version of books.csv from goodbooks-10k dataset\n",
    "users = pd.read_csv(\"Datasets\\\\Users.csv\")\n",
    "ratings= pd.read_csv(\"Datasets\\\\Ratings.csv\")\n",
    "books_large = pd.read_csv(\"Datasets\\\\Books.csv\",dtype={\"ISBN\":object},low_memory=False)\n",
    "#books3 = pd.read_csv(\"Datasets\\\\books_enriched.csv\",dtype={\"isbn\":object})\n",
    "#gr_books=pd.read_csv(\"Datasets\\\\Goodreads_books_with_genres.csv\",dtype={\"isbn\":object})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small.drop([\"book_id\",\"best_book_id\",\"description\",\"goodreads_book_id\",\"ratings_1\",\"ratings_2\",\"ratings_3\",\"ratings_4\",\"ratings_5\",\"ratings_count\",\"work_id\",\"work_ratings_count\",\"work_text_reviews_count\",\"authors_2\"],axis=1,inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of books by number of genres\n",
    "books_small['genres'].value_counts().groupby(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small[\"isbn\"].isin(ratings[\"ISBN\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small[\"isbn\"].isin(books_large[\"ISBN\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_large.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_small[\"original_title\"].isin(books_large[\"Book-Title\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_large[\"Book-Title\"].isin(books_small[\"original_title\"]).value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books=pd.merge(books_small,books_large,how=\"inner\",left_on=\"original_title\",right_on=\"Book-Title\")\n",
    "all_books=all_books[all_books['language_code'] == 'eng']\n",
    "all_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books.drop(['original_title','index','title','isbn13','books_count','publishDate','isbn','language_code','Book-Author','Year-Of-Publication','Publisher','image_url','small_image_url','Image-URL-S','Image-URL-M','Image-URL-L'],axis=1,inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books[\"ISBN\"].isin(ratings[\"ISBN\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Only Once!\n",
    "all_books[\"average_rating\"]=all_books[\"average_rating\"].apply(lambda x: x*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding column for consecutive number by duplicate book-title and year\n",
    "all_books['duplicate_count'] = all_books.groupby(['Book-Title','original_publication_year', 'authors']).cumcount()+1\n",
    "all_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['city'],users['state'],users['country']=users[\"Location\"].str.split(\",\",2).str\n",
    "users.drop(['Location'],axis=1,inplace=True, errors='ignore')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"country\"].fillna(\"Unknown\", inplace = True)\n",
    "users[\"state\"].fillna(\"Unknown\", inplace = True)\n",
    "users[\"city\"].fillna(\"Unknown\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=users[\"country\"].value_counts().where(users[\"country\"].value_counts()>7500,other=\"Other\")\n",
    "print(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"new_country\"]=users[\"country\"].apply(lambda x: x if list[x]!=\"Other\" else \"Other\")\n",
    "users[\"new_country\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list=users[\"state\"].value_counts().where(users[\"state\"].value_counts()>5000,other=\"Other\")\n",
    "users[\"new_state\"]=users[\"state\"].apply(lambda x: x if state_list[x]!=\"Other\" else \"Other\")\n",
    "users[\"new_state\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list=users[\"city\"].value_counts().where(users[\"city\"].value_counts()>1500,other=\"Other\")\n",
    "users[\"new_city\"]=users[\"city\"].apply(lambda x: x if city_list[x]!=\"Other\" else \"Other\")\n",
    "users[\"new_city\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "f=lambda x: x if x<100 else round(random.randint(24,44))\n",
    "users['Age']=users['Age'].apply(f)\n",
    "users.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.describe(include=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.drop(['city','state','country'],axis=1,inplace=True, errors='ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_books=pd.merge(ratings,all_books,how=\"inner\",left_on=\"ISBN\",right_on=\"ISBN\")\n",
    "ratings_books=pd.merge(ratings_books,all_books[all_books['duplicate_count'] == 1],how=\"inner\",left_on=[\"Book-Title\",\"original_publication_year\",\"authors\"],right_on=[\"Book-Title\",\"original_publication_year\",\"authors\"])\n",
    "ratings_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_books['ISBN']=ratings_books.apply(lambda x: x['ISBN_y'] if x['duplicate_count_x']>1 and x['duplicate_count_y']==1 else x['ISBN_x'],axis=1)\n",
    "ratings_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing when duplicate_count greater than 1\n",
    "all_books=all_books[all_books['duplicate_count'] == 1]\n",
    "#removing column duplicate_count\n",
    "all_books.drop(['duplicate_count'],axis=1,inplace=True, errors='ignore')\n",
    "all_books.reset_index(drop=True, inplace=True)\n",
    "all_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books['Book_id']=all_books.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substituting ISBN in ratings_books with book_id\n",
    "ratings_books=pd.merge(ratings_books,all_books,how=\"left\",left_on=\"ISBN\",right_on=\"ISBN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns except User-ID\tISBN\tBook-Rating\n",
    "columns_to_drop = [c for c in ratings_books.columns if c not in ['User-ID', 'Book_id','Book-Rating']]\n",
    "ratings_books.drop(columns_to_drop ,axis=1,inplace=True, errors='ignore')\n",
    "ratings_books.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_books.drop_duplicates(inplace=True)\n",
    "ratings_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_books.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings_books.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# create the MultiLabelBinarizer object\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# one-hot encode\n",
    "all_books_encoded = pd.DataFrame(mlb.fit_transform(all_books['genres']), columns=mlb.classes_, index=all_books.index)\n",
    "\n",
    "all_books_final = pd.concat([all_books, all_books_encoded], axis=1)\n",
    "all_books_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_occurences=all_books_encoded.sum().to_dict()\n",
    "genres=genre_occurences.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize genres, books are assigned a random genre from their genre list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter=lambda x: random.choice(x)\n",
    "all_books_final['genre_rnd']=all_books_final['genres'].apply(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books_final['genre_rnd'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ratings = all_books_final.merge(\n",
    "    ratings\n",
    "    .groupby(\"Book_id\", as_index=False)\n",
    "    .agg({'Book-Rating': ['count', 'mean']})\n",
    "    .flatten_cols(),\n",
    "    on='Book_id')\n",
    "\n",
    "genre_filter = alt.selection_multi(fields=['genre_rnd'])\n",
    "\n",
    "genre_chart = alt.Chart().mark_bar().encode(\n",
    "    x=\"count()\",\n",
    "    y=alt.Y('genre_rnd'),\n",
    "    color=alt.condition(\n",
    "        genre_filter,\n",
    "        alt.Color(\"genre_rnd:N\"),\n",
    "        alt.value('lightgray'))\n",
    ").properties(height=600, selection=genre_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(book_ratings[['Book_id', 'Book-Rating count', 'Book-Rating mean']]\n",
    " .sort_values('Book-Rating count', ascending=False)\n",
    " .head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_hist(field, label, filter):\n",
    "  \"\"\"Creates a layered chart of histograms.\n",
    "  The first layer (light gray) contains the histogram of the full data, and the\n",
    "  second contains the histogram of the filtered data.\n",
    "  Args:\n",
    "    field: the field for which to generate the histogram.\n",
    "    label: String label of the histogram.\n",
    "    filter: an alt.Selection object to be used to filter the data.\n",
    "  \"\"\"\n",
    "  base = alt.Chart().mark_bar().encode(\n",
    "      x=alt.X(field, bin=alt.Bin(maxbins=10), title=label),\n",
    "      y=\"count()\",\n",
    "  ).properties(\n",
    "      width=300,\n",
    "  )\n",
    "  return alt.layer(\n",
    "      base.transform_filter(filter),\n",
    "      base.encode(color=alt.value('lightgray'), opacity=alt.value(.7)),\n",
    "  ).resolve_scale(y='independent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of ratings and average rating per book.\n",
    "alt.hconcat(\n",
    "    filtered_hist('Book-Rating count', '# ratings / book', genre_filter),\n",
    "    filtered_hist('Book-Rating mean', 'mean rating', genre_filter),\n",
    "    genre_chart,\n",
    "    data=book_ratings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering-Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of user and movie ids\n",
    "user_ids = dict(zip(ratings['User-ID'].unique(), range(len(ratings['User-ID'].unique()))))\n",
    "book_ids = dict(zip(ratings['ISBN'].unique(), range(len(ratings['ISBN'].unique()))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Representation of the Ratings Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 5 code blocks contain functions taken from Google's Machine Learning Recomendation Systems Colab document. <br/>\n",
    "* The first function can be called to split the dataframe into 2 subsets, one for train and one for test. This uses the sample function rather than a train_test_split, and the fraction can be selected when called.\n",
    "* The second functon is for creating the sparse tensor from the ratings dataframe. As the rated book list contains almost 8000 entries, most books will be unrated by most users. To reduce matrix size, a sparse tensor can be utilized  <br/>\n",
    "* The third function calculates the mean squared error when given the sparse matrix A and two embedding matrices (U,V). Predictions are made by matrix multiplying the embedding tensors and mapping them to the sparse matrix space. The gather_nd function for TensorFlow combines slices from the new matrix and saves them to match the shape specified by the indices (in this example the size of the sparse matrix). A loss calculation is then performed to evaluate the prediction against the true values recorded in the sparse matrix.<br/>\n",
    "* The fourth code block defines a helper class for creating the Collaborative Filtering Model. <br/>\n",
    "* The fifth block builds the Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to split the data into training and test sets.\n",
    "def split_dataframe(df, holdout_fraction=0.1):\n",
    "  \"\"\"Splits a DataFrame into training and test sets.\n",
    "  Args:\n",
    "    df: a dataframe.\n",
    "    holdout_fraction: fraction of dataframe rows to use in the test set.\n",
    "  Returns:\n",
    "    train: dataframe for training\n",
    "    test: dataframe for testing\n",
    "  \"\"\"\n",
    "  test = df.sample(frac=holdout_fraction, replace=False)\n",
    "  train = df[~df.index.isin(test.index)]\n",
    "  return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_rating_sparse_tensor(ratings_df):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ratings_df: a pd.DataFrame with `user_id`, `ISBN` and `rating` columns.\n",
    "  Returns:\n",
    "    a tf.SparseTensor representing the ratings matrix.\n",
    "  \"\"\"\n",
    "  indices = ratings_df[['User-ID', 'Book_id']].values\n",
    "  values = ratings_df['Book-Rating'].values\n",
    "  return tf.SparseTensor(\n",
    "      indices=indices,\n",
    "      values=values,\n",
    "      dense_shape=[users.shape[0], all_books_final.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mean_square_error(sparse_ratings, user_embeddings, book_embeddings):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    sparse_ratings: A SparseTensor rating matrix, of dense_shape [N, M]\n",
    "    user_embeddings: A dense Tensor U of shape [N, k] where k is the embedding\n",
    "      dimension, such that U_i is the embedding of user i.\n",
    "    book_embeddings: A dense Tensor V of shape [M, k] where k is the embedding\n",
    "      dimension, such that V_j is the embedding of movie j.\n",
    "  Returns:\n",
    "    A scalar Tensor representing the MSE between the true ratings and the\n",
    "      model's predictions.\n",
    "  \"\"\"\n",
    "  predictions = tf.gather_nd(\n",
    "      tf.matmul(user_embeddings, book_embeddings, transpose_b=True),\n",
    "      sparse_ratings.indices)\n",
    "  loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)\n",
    "  return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CFModel helper class takes an object of 3 arguments\n",
    "* Embeddings which is a dictionary of the User_ID and Book ISBNs and the related Tensors\n",
    "* A loss calculation, in this case MSE on the sparse tensor and embedding spaces of the training dataset\n",
    "* A dictionary of metrics calculated for the train and test datasets converted to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFModel(object):\n",
    "  \"\"\"Simple class that represents a collaborative filtering model\"\"\"\n",
    "  def __init__(self, embedding_vars, loss, metrics=None):\n",
    "    \"\"\"Initializes a CFModel.\n",
    "    Args:\n",
    "      embedding_vars: A dictionary of tf.Variables.\n",
    "      loss: A float Tensor. The loss to optimize.\n",
    "      metrics: optional list of dictionaries of Tensors. The metrics in each\n",
    "        dictionary will be plotted in a separate figure during training.\n",
    "    \"\"\"\n",
    "    self._embedding_vars = embedding_vars\n",
    "    self._loss = loss\n",
    "    self._metrics = metrics\n",
    "    self._embeddings = {k: None for k in embedding_vars}\n",
    "    self._session = None\n",
    "\n",
    "  @property\n",
    "  def embeddings(self):\n",
    "    \"\"\"The embeddings dictionary.\"\"\"\n",
    "    return self._embeddings\n",
    "\n",
    "  def train(self, num_iterations=100, learning_rate=1.0, plot_results=True,\n",
    "            optimizer=tf.train.GradientDescentOptimizer):\n",
    "    \"\"\"Trains the model.\n",
    "    Args:\n",
    "      iterations: number of iterations to run.\n",
    "      learning_rate: optimizer learning rate.\n",
    "      plot_results: whether to plot the results at the end of training.\n",
    "      optimizer: the optimizer to use. Default to GradientDescentOptimizer.\n",
    "    Returns:\n",
    "      The metrics dictionary evaluated at the last iteration.\n",
    "    \"\"\"\n",
    "    with self._loss.graph.as_default():\n",
    "      opt = optimizer(learning_rate)\n",
    "      train_op = opt.minimize(self._loss)\n",
    "      local_init_op = tf.group(\n",
    "          tf.variables_initializer(opt.variables()),\n",
    "          tf.local_variables_initializer())\n",
    "      if self._session is None:\n",
    "        self._session = tf.Session()\n",
    "        with self._session.as_default():\n",
    "          self._session.run(tf.global_variables_initializer())\n",
    "          self._session.run(tf.tables_initializer())\n",
    "          tf.train.start_queue_runners()\n",
    "\n",
    "    with self._session.as_default():\n",
    "      local_init_op.run()\n",
    "      iterations = []\n",
    "      metrics = self._metrics or ({},)\n",
    "      metrics_vals = [collections.defaultdict(list) for _ in self._metrics]\n",
    "\n",
    "      # Train and append results.\n",
    "      for i in range(num_iterations + 1):\n",
    "        _, results = self._session.run((train_op, metrics))\n",
    "        if (i % 10 == 0) or i == num_iterations:\n",
    "          print(\"\\r iteration %d: \" % i + \", \".join(\n",
    "                [\"%s=%f\" % (k, v) for r in results for k, v in r.items()]),\n",
    "                end='')\n",
    "          iterations.append(i)\n",
    "          for metric_val, result in zip(metrics_vals, results):\n",
    "            for k, v in result.items():\n",
    "              metric_val[k].append(v)\n",
    "\n",
    "      for k, v in self._embedding_vars.items():\n",
    "        self._embeddings[k] = v.eval()\n",
    "\n",
    "      if plot_results:\n",
    "        # Plot the metrics.\n",
    "        num_subplots = len(metrics)+1\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(num_subplots*10, 8)\n",
    "        for i, metric_vals in enumerate(metrics_vals):\n",
    "          ax = fig.add_subplot(1, num_subplots, i+1)\n",
    "          for k, v in metric_vals.items():\n",
    "            ax.plot(iterations, v, label=k)\n",
    "          ax.set_xlim([1, num_iterations])\n",
    "          ax.legend()\n",
    "      return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CFM function takes as input the ratings dataframe, the dimensions of the embedding vectors, and the standard deviation of the random initial embeddings <br/>\n",
    "The code first creates 2 subsets of data for the test and train splits <br/>\n",
    "It then creates the sparse tensor for each split <br/>\n",
    "The embedding spaces U and V are created using the provided dimensions and a normal distribution from the train data split <br/>\n",
    "Loss is calculated for MSE on the sparse tensor and embedding spaces and saved to a metrics dictionary\n",
    "Finally the embeddings are labelled and saved to a dictionary. \n",
    "The build_model function returns the model created by the helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(ratings, embedding_dim=3, init_stddev=1.):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ratings: a DataFrame of the ratings\n",
    "    embedding_dim: the dimension of the embedding vectors.\n",
    "    init_stddev: float, the standard deviation of the random initial embeddings.\n",
    "  Returns:\n",
    "    model: a CFModel.\n",
    "  \"\"\"\n",
    "  # Split the ratings DataFrame into train and test.\n",
    "  train_ratings, test_ratings = split_dataframe(ratings)\n",
    "  # SparseTensor representation of the train and test datasets.\n",
    "  A_train = build_rating_sparse_tensor(train_ratings)\n",
    "  A_test = build_rating_sparse_tensor(test_ratings)\n",
    "  # Initialize the embeddings using a normal distribution.\n",
    "  U = tf.Variable(tf.random_normal(\n",
    "      [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))\n",
    "  V = tf.Variable(tf.random_normal(\n",
    "      [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))\n",
    "  train_loss = sparse_mean_square_error(A_train, U, V)\n",
    "  test_loss = sparse_mean_square_error(A_test, U, V)\n",
    "  metrics = {\n",
    "      'train_error': train_loss,\n",
    "      'test_error': test_loss\n",
    "  }\n",
    "  embeddings = {\n",
    "      \"user_id\": U,\n",
    "      \"ISBN\": V\n",
    "  }\n",
    "  return CFModel(embeddings, train_loss, [metrics])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Collaborative Filtering Matrix Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_model=build_model(ratings, embedding_dim=3, init_stddev=0.5)\n",
    "#matrix_model.train(num_iterations=100, learning_rate=0.1, plot_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering-autoencoder Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation System"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
